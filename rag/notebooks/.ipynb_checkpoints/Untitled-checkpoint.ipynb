{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1f1ae-53b7-4175-a20a-5d8d7971b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd93253d-a1f2-4010-932b-3c0977489400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_eval(prompt):\n",
    "    # Simulating different types of responses\n",
    "    response_types = ['highly relevant', 'somewhat relevant', 'irrelevant']\n",
    "    scores = {'highly relevant': 3, 'somewhat relevant': 2, 'irrelevant': 1}\n",
    "\n",
    "    # Perform multiple random trials\n",
    "    trials = 100\n",
    "    total_score = 0\n",
    "    for _ in range(trials):\n",
    "        response = random.choice(response_types)\n",
    "        total_score += scores[response]\n",
    "\n",
    "    # Average score represents the evaluation\n",
    "    return total_score / trials\n",
    "\n",
    "def elo_eval(prompt, base_rating=1500):\n",
    "    # Simulate the outcome of the prompt against standard criteria\n",
    "    # Here, we randomly decide if the prompt 'wins', 'loses', or 'draws'\n",
    "    outcomes = ['win', 'loss', 'draw']\n",
    "    outcome = random.choice(outcomes)\n",
    "\n",
    "    # Elo rating formula parameters\n",
    "    K = 30  # Maximum change in rating\n",
    "    R_base = 10 ** (base_rating / 400)\n",
    "    R_opponent = 10 ** (1600 / 400)  # Assuming a fixed opponent rating\n",
    "    expected_score = R_base / (R_base + R_opponent)\n",
    "\n",
    "    # Calculate the new rating based on the outcome\n",
    "    actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]\n",
    "    new_rating = base_rating + K * (actual_score - expected_score)\n",
    "\n",
    "    return new_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e207cb-7e97-49ce-9fa9-7b5bac4a21eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elo_ratings_func(prompts, elo_ratings, K=30, opponent_rating=1600):\n",
    "    \"\"\"\n",
    "    Update Elo ratings for a list of prompts based on simulated outcomes.\n",
    "\n",
    "    Parameters:\n",
    "    prompts (list): List of prompts to be evaluated.\n",
    "    elo_ratings (dict): Current Elo ratings for each prompt.\n",
    "    K (int): Maximum change in rating.\n",
    "    opponent_rating (int): Fixed rating of the opponent for simulation.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated Elo ratings.\n",
    "    \"\"\"\n",
    "\n",
    "    for prompt in prompts:\n",
    "        # Simulate an outcome against the standard criteria or another prompt\n",
    "        outcome = random.choice(['win', 'loss', 'draw'])\n",
    "\n",
    "        # Calculate the new rating based on the outcome\n",
    "        actual_score = {'win': 1, 'loss': 0, 'draw': 0.5}[outcome]\n",
    "        R_base = 10 ** (elo_ratings[prompt] / 400)\n",
    "        R_opponent = 10 ** (opponent_rating / 400)\n",
    "        expected_score = R_base / (R_base + R_opponent)\n",
    "        elo_ratings[prompt] += K * (actual_score - expected_score)\n",
    "\n",
    "    return elo_ratings\n",
    "\n",
    "# Example usage\n",
    "prompts = [\n",
    "            \"Who founded OpenAI?\", \n",
    "            \"What was the initial goal of OpenAI?\",\n",
    "            \"What did OpenAI release in 2016?\", \n",
    "            \"What project did OpenAI showcase in 2018?\",\n",
    "            \"How did the AI agents in OpenAI Five work together?\"\n",
    "                ]\n",
    "elo_ratings = {prompt: 1500 for prompt in prompts}  # Initial ratings\n",
    "\n",
    "# Conduct multiple rounds of evaluation\n",
    "for _ in range(10):  # Number of rounds\n",
    "    elo_ratings = elo_ratings_func(prompts, elo_ratings)\n",
    "\n",
    "# Sort prompts by their final Elo ratings\n",
    "sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)\n",
    "\n",
    "# Print the ranked prompts\n",
    "for prompt in sorted_prompts:\n",
    "    print(f\"{prompt}: {elo_ratings[prompt]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee3d42-0093-45c7-9bb4-d2e73143b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "How did the AI agents in OpenAI Five work together?: 1548.3188869745466\n",
    "What did OpenAI release in 2016?: 1533.8606319951714\n",
    "Who founded OpenAI?: 1533.4486254910844\n",
    "What project did OpenAI showcase in 2018?: 1528.63542805577\n",
    "What was the initial goal of OpenAI?: 1498.6276146134444"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b7c4a-7cab-4038-a479-e242603cabd6",
   "metadata": {},
   "source": [
    "#### prompts evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1318d8b2-d2ce-433d-b558-fc60ccd7438b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"What was the initial goal of OpenAI?\": 1583.6551603182484\n",
    "This prompt has the highest rating,  suggesting it was evaluated as the most relevant, accurate, or valuable.\n",
    "\"Who founded OpenAI?\": 1550.8315837034786\n",
    "This prompt also performed well, but slightly less so than the first one.\n",
    "\"What project did OpenAI showcase in 2018?\": 1524.894352475904 Moderate\n",
    "\"What did OpenAI release in 2016?\": 1518.8441077283887\n",
    "These prompts have lower ratings, indicating they were evaluated as less relevant or valuable compared to the top-rated prompts.\n",
    "\"How did the AI agents in OpenAI Five work together?\": 1501.4300442180024\n",
    "This prompt is closer to the baseline rating, suggesting its performance was near average in your evaluation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da9e57-34c0-4f11-8efd-2107f0c484ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65515b-d95a-4d0b-96a4-4bf224a57b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(main_prompt, test_cases):\n",
    "    evaluations = {}\n",
    "\n",
    "    # Evaluate the main prompt using Monte Carlo and Elo methods\n",
    "    evaluations['main_prompt'] = {\n",
    "        'Monte Carlo Evaluation': monte_carlo_eval(main_prompt),\n",
    "        'Elo Rating Evaluation': elo_eval(main_prompt)\n",
    "    }\n",
    "\n",
    "    # Evaluate each test case\n",
    "    for idx, test_case in enumerate(test_cases):\n",
    "        evaluations[f'test_case_{idx+1}'] = {\n",
    "            'Monte Carlo Evaluation': monte_carlo_eval(test_case),\n",
    "            'Elo Rating Evaluation': elo_eval(test_case)\n",
    "        }\n",
    "\n",
    "    return evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c1ab0-d948-4725-8138-47e15ac130bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt = \"why we use OepenAI?\"\n",
    "test_cases = [\"Who founded OpenAI?\", \n",
    "                \"What was the initial goal of OpenAI?\",\n",
    "                \"What did OpenAI release in 2016?\", \n",
    "                \"What project did OpenAI showcase in 2018?\",\n",
    "                \"How did the AI agents in OpenAI Five work together?\"\n",
    "                ]\n",
    "result = evaluate_prompt(main_prompt, test_cases)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2271bb-1928-4c66-ad9f-10ba82433ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'main_prompt': {'Monte Carlo Evaluation': 1.93, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_1': {'Monte Carlo Evaluation': 1.97, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_2': {'Monte Carlo Evaluation': 1.99, 'Elo Rating Evaluation': 1489.2019499940866}, 'test_case_3': {'Monte Carlo Evaluation': 2.01, 'Elo Rating Evaluation': 1519.2019499940866}, 'test_case_4': {'Monte Carlo Evaluation': 1.93, 'Elo Rating Evaluation': 1504.2019499940866}, 'test_case_5': {'Monte Carlo Evaluation': 2.0, 'Elo Rating Evaluation': 1504.2019499940866}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f12e08-28b6-4f86-b6b5-4030735f9cb4",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "##### 1. Monte Carlo Evaluation:\n",
    "Scores Range: From 1 to 3, with higher scores indicating greater relevance or quality of the prompt.\n",
    "###### Interpretation:\n",
    "1.94 (Main Prompt): Slightly below average relevance or quality.\n",
    "2.06, 2.02, 1.89, 1.98, 2.03 (Test Cases): Scores around 2 suggest moderate relevance or quality. The variation indicates some test cases are deemed slightly more relevant or higher quality than others.\n",
    "#### 2. Elo Rating Evaluation:\n",
    "Base Rating: Usually starts at 1500, with changes based on the 'performance' of the prompt against a set of standards.\n",
    "Higher than 1500: Indicates the prompt performed better than average.\n",
    "Lower than 1500: Indicates the prompt performed worse than average.\n",
    "###### Interpretation:\n",
    "1489.20 (Main Prompt): Slightly below the average performance.\n",
    "1519.20 (Test Cases 1, 2, 4, 5): These prompts are rated above the average, suggesting better performance.\n",
    "1504.20 (Test Case 3): Slightly above average performance.\n",
    "#### Overall Interpretation:\n",
    "Main Prompt: Both evaluations suggest that the main prompt is slightly below average in terms of relevance and quality.\n",
    "Test Cases: Generally, the test cases are rated as average or slightly above average in both relevance and quality. Test Cases 1, 2, 4, and 5 seem to perform particularly well in the Elo evaluation, indicating they might be more effective or well-structured prompts compared to the main prompt and Test Case 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7332654-546b-4d75-9be8-a5265ef253aa",
   "metadata": {},
   "source": [
    "### RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4caa961-3802-4488-bccb-3fd55557ebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter  \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Weaviate\n",
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "# \n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25692e67-6480-4d20-adfc-3da230d1ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "def data_loader(file_path= '../prompts/context.txt'):\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Chunk the data\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff800cf6-14fa-4e44-91a0-a72b500ee951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever(chunks):\n",
    "\n",
    "  # Load OpenAI API key from .env file\n",
    "  load_dotenv(find_dotenv())\n",
    "\n",
    "  # Setup vector database\n",
    "  client = weaviate.Client(\n",
    "    embedded_options = EmbeddedOptions()\n",
    "  )\n",
    "\n",
    "  # Populate vector database\n",
    "  vectorstore = Weaviate.from_documents(\n",
    "      client = client,    \n",
    "      documents = chunks,\n",
    "      embedding = OpenAIEmbeddings(),\n",
    "      by_text = False\n",
    "  )\n",
    "\n",
    "  # Define vectorstore as retriever to enable semantic search\n",
    "  retriever = vectorstore.as_retriever()\n",
    "  return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28da04-bc2e-4f06-be4d-bca724205b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chunk\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46b834-5e61-4c53-be58-29cba65029ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks =  data_loader()\n",
    "retriever = create_retriever(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf33061-a061-4504-8878-15afa539080a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
