{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49f0609-60c9-43a5-8316-63a19c996080",
   "metadata": {},
   "source": [
    "#### [LangChain Handbook](https://pinecone.io/learn/langchain)\n",
    "\n",
    "# Retrieval Augmentation\n",
    "\n",
    "Large Language Models (LLMs) have a data freshness problem. The most powerful LLMs in the world, like GPT-4, have no idea about recent world events.\n",
    "\n",
    "The world of LLMs is frozen in time. Their world exists as a static snapshot of the world as it was within their training data.\n",
    "\n",
    "A solution to this problem is *retrieval augmentation*. The idea behind this is that we retrieve relevant information from an external knowledge base and give that information to our LLM. In this notebook we will learn how to do that.\n",
    "\n",
    "visit the notebook](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/full-link.svg)](https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/05-langchain-retrieval-augmentation.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585a3f4-fcb8-4325-84b4-0b0b9ad9461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "To begin, we must install the prerequisite libraries that we will be using in this notebook.\n",
    "# !pip install -qU \\\n",
    "#   langchain==0.1.1 \\\n",
    "#   langchain-community==0.0.13 \\\n",
    "#   openai==0.27.7 \\\n",
    "#   tiktoken==0.4.0 \\\n",
    "#   pinecone-client==3.0.0 \\\n",
    "#   pinecone-datasets==0.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc53eeb-591f-432a-8703-caacb2d457b6",
   "metadata": {},
   "source": [
    "#### Building the Knowledge Base\n",
    "\n",
    "We will download a pre-embedding dataset from `pinecone-datasets`. Allowing us to skip the embedding and preprocessing steps, \n",
    "if you'd rather work through those steps you can find the [full notebook here](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/langchain-retrieval-augmentation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3df91-ad7a-4bd6-b5d9-15f4564c8ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone_datasets\n",
    "\n",
    "dataset = pinecone_datasets.load_dataset('wikipedia-simple-text-embedding-ada-002-100K')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d92cc9-42ae-4654-8bff-b09f944cfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5930b290-ea70-468c-bb13-010d8def0acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the dataset for upsert and reduce what we use to a subset of dataset \n",
    "# we drop sparse_values as they are not needed for this example\n",
    "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
    "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
    "# we will use rows of the dataset up to index 30_000\n",
    "dataset.documents.drop(dataset.documents.index[30_000:], inplace=True)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b0f5d7-99b0-4422-8ae5-37e54ef95b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize pinecone with vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e66d1-6a18-4135-9994-bd5b475c9e98",
   "metadata": {},
   "source": [
    "#### vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c649bcf-5c97-4b2a-9274-bd690309a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create vector database a free API key from pinecone\n",
    "#choose whether to serveless or pod-based index\n",
    "import os\n",
    "\n",
    "use_serverless = os.environ.get(\"USE_SERVERLESS\", \"False\").lower() == \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f616b-e4cd-44ae-abf6-063d55e36767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize connection with pinecone with API key\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pc.io)\n",
    "api_key = os.environ.get('PINECONE_API_KEY') or 'PINECONE_API_KEY'\n",
    "environment = os.environ.get('PINECONE_ENVIRONMENT') or 'PINECONE_ENVIRONMENT'\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d8ea18-dfc6-4b07-8e6d-cda98f5f07d9",
   "metadata": {},
   "source": [
    "##### setup the index specification,which will allow us to define the cloud provider and region where we want to deploy the index [available providers and regions here](https://docs.pinecone.io/docs/projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b298912-523d-4be1-bd45-7d0c273292e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec, PodSpec\n",
    "\n",
    "if use_serverless:\n",
    "    spec = ServerlessSpec(cloud='aws', region='us-west-2')\n",
    "else:\n",
    "    spec = PodSpec(environment=environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b96996-07af-4eda-8536-3d2d355f2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'langchain-retrieval-augmentation-fast'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0e24fd-dd8e-4672-8362-540a57b4492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# we create a new index\n",
    "pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "# wait for index to be initialized\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444fba7-06ea-4d9b-8154-ab47ca88cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the index \n",
    "index = pc.Index(index_name)\n",
    "# wait a moment for connection\n",
    "time.sleep(1)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9777142-8313-4d61-9b6d-20837cc42787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We should see that the new Pinecone index has a `total_vector_count` of `0`, as we haven't added any vectors yet.\n",
    "#Now we upsert the data to Pinecone:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a1317-6e03-4612-81aa-2edca9f286d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataset.iter_documents(batch_size=100):\n",
    "    index.upsert(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf01c0-66e0-4033-b669-eafcd960fe56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We've now indexed everything. We can check the number of vectors in our index like so:\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb04096-2884-420f-9342-346b5e481452",
   "metadata": {},
   "source": [
    "### Creating vector store to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2b4bb-598f-4e2e-8816-b9c910965fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#after creating index then switch to langchain to initialize langchain vectorstore\n",
    "#also we will also need a LangChain embedding object, which we initialize like so:\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# get openai api key from platform.openai.com\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37922f17-6664-420e-8a65-72e424457f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize vector store \n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d7e96-4a21-4d28-a45a-d50911229bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#query vector store using vectot.similarity_search:\n",
    "query = \"who was Benito Mussolini?\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce25898a-5a73-4539-a6c4-72989cfdb12d",
   "metadata": {},
   "source": [
    "##### generative question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfc71f-ed47-4459-adb7-cf062e010a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize RetrievalQA object\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a5d27-0fa1-4ada-a35e-7def1438881d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06734d1c-54c5-4400-873f-18bdccd72988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include the sources of information that the LLM is using to answer question,using a slightly different version of `RetrievalQA` called `RetrievalQAWithSourcesChain`:\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec65f83-3780-42a9-9d91-8b13c815bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_sources(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589d9c3-d97c-4db1-9a84-3df73af41f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we answer the question being asked, *and* return the source of this information being used by the LLM.\n",
    "#Once done, we can delete the index to save resources.\n",
    "pc.delete_index(index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
