{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52315566-d649-427e-a10e-6e3c4ad2ab09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "sys.path.append(os.path.abspath(os.path.join('../utility')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7206a81a-715b-4127-bbbf-63ae431213b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ranking_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mranking_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elo_ratings_func, evaluate_prompt\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ranking_utils'"
     ]
    }
   ],
   "source": [
    "from ranking_utils import elo_ratings_func, evaluate_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3089a2db-67ec-441c-8359-a1fade8e367c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'elo_ratings_func' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Conduct multiple rounds of evaluation\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Number of rounds\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     elo_ratings \u001b[38;5;241m=\u001b[39m \u001b[43melo_ratings_func\u001b[49m(prompts, elo_ratings)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Sort prompts by their final Elo ratings\u001b[39;00m\n\u001b[1;32m     16\u001b[0m sorted_prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(prompts, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: elo_ratings[x], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'elo_ratings_func' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "prompts = [\n",
    "            \"Who founded OpenAI?\", \n",
    "            \"What was the initial goal of OpenAI?\",\n",
    "            \"What did OpenAI release in 2016?\", \n",
    "            \"What project did OpenAI showcase in 2018?\",\n",
    "            \"How did the AI agents in OpenAI Five work together?\"\n",
    "                ]\n",
    "elo_ratings = {prompt: 1500 for prompt in prompts}  # Initial ratings\n",
    "\n",
    "# Conduct multiple rounds of evaluation\n",
    "for _ in range(10):  # Number of rounds\n",
    "    elo_ratings = elo_ratings_func(prompts, elo_ratings)\n",
    "\n",
    "# Sort prompts by their final Elo ratings\n",
    "sorted_prompts = sorted(prompts, key=lambda x: elo_ratings[x], reverse=True)\n",
    "\n",
    "# Print the ranked prompts\n",
    "for prompt in sorted_prompts:\n",
    "    print(f\"{prompt}: {elo_ratings[prompt]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c154df-daf4-4c9d-b3be-fae7e944006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_prompt = \"why we use OepenAI?\"\n",
    "test_cases = [\"Who founded OpenAI?\", \n",
    "                \"What was the initial goal of OpenAI?\",\n",
    "                \"What did OpenAI release in 2016?\", \n",
    "                \"What project did OpenAI showcase in 2018?\",\n",
    "                \"How did the AI agents in OpenAI Five work together?\"\n",
    "                ]\n",
    "result = evaluate_prompt(main_prompt, test_cases)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
